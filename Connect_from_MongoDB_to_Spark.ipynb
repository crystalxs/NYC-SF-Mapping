{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In SageMaker, you might need to configure to load the mongo-spark-connector package and kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install s2sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mapsplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/anaconda2/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pyspark_submit_args = '--packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0 pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://54.212.226.251:27017/msds697proj.nyc_taxi\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"End_Lat\",DoubleType(),True),\n",
    " StructField(\"End_Lon\",DoubleType(),True),\n",
    " StructField(\"Fare_Amt\",DoubleType(),True),\n",
    " StructField(\"Passenger_Count\",IntegerType(),True),\n",
    " StructField(\"Payment_Type\",StringType(),True),\n",
    " StructField(\"Rate_Code\",StringType(),True),\n",
    " StructField(\"Start_Lat\",DoubleType(),True),\n",
    " StructField(\"Start_Lon\",DoubleType(),True),\n",
    " StructField(\"Tip_Amt\",DoubleType(),True),\n",
    " StructField(\"Tolls_Amt\",DoubleType(),True),\n",
    " StructField(\"Total_Amt\",DoubleType(),True),\n",
    " StructField(\"Trip_Distance\",DoubleType(),True),\n",
    " StructField(\"Trip_Dropoff_DateTime\",StringType(),True),\n",
    " StructField(\"Trip_Pickup_DateTime\",StringType(),True),\n",
    " StructField(\"_id\",StructType([StructField(\"oid\",StringType(),True)]),True),\n",
    " StructField(\"mta_tax\",StringType(),True),\n",
    " StructField(\"store_and_forward\",StringType(),True),\n",
    " StructField(\"surcharge\",DoubleType(),True),\n",
    " StructField(\"vendor_name\",StringType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load(schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- End_Lat: double (nullable = true)\n",
      " |-- End_Lon: double (nullable = true)\n",
      " |-- Fare_Amt: double (nullable = true)\n",
      " |-- Passenger_Count: integer (nullable = true)\n",
      " |-- Payment_Type: string (nullable = true)\n",
      " |-- Rate_Code: string (nullable = true)\n",
      " |-- Start_Lat: double (nullable = true)\n",
      " |-- Start_Lon: double (nullable = true)\n",
      " |-- Tip_Amt: double (nullable = true)\n",
      " |-- Tolls_Amt: double (nullable = true)\n",
      " |-- Total_Amt: double (nullable = true)\n",
      " |-- Trip_Distance: double (nullable = true)\n",
      " |-- Trip_Dropoff_DateTime: string (nullable = true)\n",
      " |-- Trip_Pickup_DateTime: string (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- store_and_forward: string (nullable = true)\n",
      " |-- surcharge: double (nullable = true)\n",
      " |-- vendor_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nyc_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_cols(df):\n",
    "    for col in df.columns:\n",
    "        new_col = col.lower()\n",
    "        if new_col != col:\n",
    "            df = df.withColumnRenamed(col, new_col)\n",
    "    return df\n",
    "\n",
    "def toDatetime(df, col_name):\n",
    "    df = df.withColumn(col_name + \"_2\", to_timestamp(nyc_df[col_name], 'yyyy-MM-dd HH:mm:ss'))\n",
    "    df = df.drop(col_name).withColumnRenamed(col_name + \"_2\", col_name)\n",
    "    return df\n",
    "\n",
    "nyc_df = lowercase_cols(nyc_df)\n",
    "nyc_df = toDatetime(nyc_df, \"trip_pickup_datetime\")\n",
    "nyc_df = toDatetime(nyc_df, \"trip_dropoff_datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract day of week, add \"is_weekend\" flag when trip starts OR ends during the weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_df = nyc_df.withColumn('dow_dropoff', date_format('trip_dropoff_datetime', 'u').cast(IntegerType()))\\\n",
    ".withColumn('strdow_dropoff', date_format('trip_dropoff_datetime', 'E'))\\\n",
    ".withColumn('dow_pickup', date_format('trip_pickup_datetime', 'u').cast(IntegerType()))\\\n",
    ".withColumn('strdow_pickup', date_format('trip_pickup_datetime', 'E'))\\\n",
    ".withColumn('hour_pickup', date_format('trip_pickup_datetime', 'H').cast(IntegerType()))\\\n",
    ".withColumn('hour_dropoff', date_format('trip_dropoff_datetime', 'H').cast(IntegerType()))\n",
    "\n",
    "nyc_df = nyc_df.withColumn(\"is_weekend\", when((nyc_df.dow_pickup >= 6) | (nyc_df.dow_dropoff >= 6), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_level = 13\n",
    "\n",
    "def coord_to_id_fun(x, cell_level=cell_level):\n",
    "    lat, lon = x\n",
    "    from s2sphere import CellId, LatLng\n",
    "    cell_id = CellId.from_lat_lng(LatLng.from_degrees(lat, lon))\\\n",
    "    .parent(cell_level).to_token()\n",
    "    return cell_id\n",
    "\n",
    "def get_corners(s2CellId_str, cell_level=cell_level+1, cluster=1):\n",
    "    from s2sphere import CellId, LatLng, Cell\n",
    "    c1 = Cell(CellId(int(s2CellId_str,16)<<(60 - 2*cell_level)))\n",
    "    v0 = LatLng.from_point(c1.get_vertex(0)) # lat/lon of upper/left corner\n",
    "    v1 = LatLng.from_point(c1.get_vertex(1)) # lat/lon of lower/left corner\n",
    "    v2 = LatLng.from_point(c1.get_vertex(2)) # lat/lon of lower/right corner\n",
    "    v3 = LatLng.from_point(c1.get_vertex(3)) # lat/lon of upper/right corner\n",
    "    return ((v0.lat().degrees, v0.lng().degrees, cluster),\n",
    "            (v1.lat().degrees, v1.lng().degrees, cluster),\n",
    "            (v2.lat().degrees, v2.lng().degrees, cluster),\n",
    "            (v3.lat().degrees, v3.lng().degrees, cluster))\n",
    "\n",
    "def distance_fun(x):\n",
    "    import math\n",
    "    lat1, lon1, lat2, lon2 = x\n",
    "    radius = 6371 # km\n",
    "\n",
    "    dlat = math.radians(lat2-lat1)\n",
    "    dlon = math.radians(lon2-lon1)\n",
    "    a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(lat1)) \\\n",
    "        * math.cos(math.radians(lat2)) * math.sin(dlon/2) * math.sin(dlon/2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    d = radius * c\n",
    "    return d\n",
    "\n",
    "coord_to_id = udf(lambda x: coord_to_id_fun(x), StringType())\n",
    "distance = udf(lambda x: distance_fun(x), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[end_lat: double, end_lon: double, fare_amt: double, passenger_count: int, payment_type: string, rate_code: string, start_lat: double, start_lon: double, tip_amt: double, tolls_amt: double, total_amt: double, trip_distance: double, _id: struct<oid:string>, mta_tax: string, store_and_forward: string, surcharge: double, vendor_name: string, trip_pickup_datetime: timestamp, trip_dropoff_datetime: timestamp, dow_dropoff: int, strdow_dropoff: string, dow_pickup: int, strdow_pickup: string, hour_pickup: int, hour_dropoff: int, is_weekend: int, start_cell_id: string, end_cell_id: string, distance_km: float]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyc_df = nyc_df.withColumn(\"start_cell_id\", coord_to_id(array(\"start_lat\", \"start_lon\")))\n",
    "nyc_df = nyc_df.withColumn(\"end_cell_id\", coord_to_id(array(\"end_lat\", \"end_lon\")))\n",
    "nyc_df = nyc_df.withColumn(\"distance_km\", distance(array(\"start_lat\", \"start_lon\",\"end_lat\", \"end_lon\")))\n",
    "nyc_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nyc_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common outbound cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_cell_id = nyc_df.groupBy(\"end_cell_id\").count().orderBy(\"count\", ascending=False).first().asDict()[\"end_cell_id\"]\n",
    "unpopular_cell_id = nyc_df.groupBy(\"end_cell_id\").count().orderBy(\"count\", ascending=True).first().asDict()[\"end_cell_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gather features by cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_column(df, group_col, pv_col, prefix):\n",
    "    orig_cols = set(df.columns)\n",
    "    #nyc_df = nyc_df.groupBy([\"start_cell_id\", \"dow_pickup\"]).count()#.filter(nyc_df.start_cell_id == \"89c243dc\").show(5)\n",
    "    new_df = df.groupBy(group_col).pivot(pv_col).count()\n",
    "    new_cols = new_df.columns\n",
    "    new_cols = [c if c in orig_cols else prefix + c for c in new_cols]\n",
    "    new_df = new_df.toDF(*new_cols)\n",
    "    new_df = new_df.na.fill(0, list(set(new_cols) - orig_cols))\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_dow_pickup_df = pivot_column(nyc_df, group_col = \"start_cell_id\", pv_col = \"dow_pickup\", prefix = \"dow_pickup_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_dow_pickup_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_dow_dropoff_df = pivot_column(nyc_df, group_col = \"end_cell_id\", pv_col = \"dow_dropoff\", prefix = \"dow_dropoff_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dfs(dfs):\n",
    "    first_df = None\n",
    "    for df in dfs:\n",
    "        if first_df is None:\n",
    "            first_df = df\n",
    "        else:\n",
    "            first_df = first_df.join(df, \"cell_id\", \"left_outer\")\n",
    "    return first_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_hr_pickup_wkday_df = pivot_column(nyc_df.filter(nyc_df.dow_pickup<6), \n",
    "                                                           group_col = \"start_cell_id\", \n",
    "                                                           pv_col = \"hour_pickup\", \n",
    "                                                           prefix = \"hr_pickup_wkday\")\n",
    "\n",
    "nyc_hr_dropoff_wkday_df = pivot_column(nyc_df.filter(nyc_df.dow_dropoff<6), \n",
    "                                                           group_col = \"end_cell_id\", \n",
    "                                                           pv_col = \"hour_dropoff\", \n",
    "                                                           prefix = \"hr_dropoff_wkday\")\n",
    "\n",
    "nyc_hr_pickup_wkend_df = pivot_column(nyc_df.filter(nyc_df.dow_pickup>=6), \n",
    "                                                           group_col = \"start_cell_id\", \n",
    "                                                           pv_col = \"hour_pickup\", \n",
    "                                                           prefix = \"hr_pickup_wkend\")\n",
    "\n",
    "nyc_hr_dropoff_wkend_df = pivot_column(nyc_df.filter(nyc_df.dow_dropoff>=6), \n",
    "                                                           group_col = \"end_cell_id\", \n",
    "                                                           pv_col = \"hour_dropoff\", \n",
    "                                                           prefix = \"hr_dropoff_wkend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_pickup_km_df = nyc_df.groupBy(\"start_cell_id\").agg(avg(\"distance_km\"),stddev(\"distance_km\"))\\\n",
    "                    .withColumnRenamed(\"avg(distance_km)\", \"avg_pickup_km\")\\\n",
    "                    .withColumnRenamed(\"stddev_samp(distance_km)\", \"std_pickup_km\")\\\n",
    "                    .withColumnRenamed(\"start_cell_id\", \"cell_id\")\n",
    "    \n",
    "nyc_dropoff_km_df = nyc_df.groupBy(\"end_cell_id\").agg(avg(\"distance_km\"),stddev(\"distance_km\"))\\\n",
    "                    .withColumnRenamed(\"avg(distance_km)\", \"avg_dropoff_km\")\\\n",
    "                    .withColumnRenamed(\"stddev_samp(distance_km)\", \"std_dropoff_km\")\\\n",
    "                    .withColumnRenamed(\"end_cell_id\", \"cell_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_cell_df = merge_dfs([nyc_dow_pickup_df, nyc_dow_dropoff_df, \n",
    "          nyc_hr_pickup_wkday_df, nyc_hr_dropoff_wkday_df,\n",
    "          nyc_hr_pickup_wkend_df, nyc_hr_dropoff_wkend_df,\n",
    "          nyc_pickup_km_df, nyc_dropoff_km_df\n",
    "         ])\n",
    "\n",
    "nyc_cell_df = nyc_cell_df.na.fill(0)\n",
    "\n",
    "# nyc_cell_df.orderBy(\"hr_dropoff_wkend12\", ascending=True).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_cell_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [c for c in nyc_cell_df.columns if c != \"cell_id\"]\n",
    "\n",
    "for c in feature_columns:\n",
    "    nyc_cell_df = nyc_cell_df.withColumn(c+\"_f\", nyc_cell_df[c].cast(FloatType()))\n",
    "    nyc_cell_df = nyc_cell_df.drop(c)\n",
    "    nyc_cell_df = nyc_cell_df.withColumnRenamed(c+\"_f\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_columns = [\"distance_km\", \"dow_pickup\", \"dow_dropoff\", \"hour_pickup\", \"hour_dropoff\", \"is_weekend\"]\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=feature_columns)\n",
    "vectorized_df = va.transform(nyc_cell_df)\n",
    "vectorized_df = vectorized_df.select(\"features\")\n",
    "vectorized_df.cache()\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\",\\\n",
    "         outputCol=\"scaledFeatures\")\n",
    "scalerModel =  scaler.fit(vectorized_df)\n",
    "vectorized_df = scalerModel.transform(vectorized_df)\n",
    "\n",
    "vectorized_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mapsplotlib import mapsplot as mplt\n",
    "import constants as c\n",
    "mplt.register_api_key(c.google_maps_key)\n",
    "\n",
    "def show_cell(cell_id, cell_level=cell_level+1):\n",
    "    map_data = get_corners(cell_id, cell_level=14)\n",
    "    map_df = pd.DataFrame(np.array(map_data), columns=[\"latitude\", \"longitude\", \"cluster\"])\n",
    "    map_df.cluster = map_df.cluster.astype(np.int)\n",
    "    mplt.polygons(map_df['latitude'], map_df['longitude'], map_df['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cell('89c258fc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SF Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://54.212.226.251:27017/msds697proj.sf_taxi\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
